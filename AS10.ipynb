{"cells":[{"cell_type":"markdown","metadata":{"id":"C9uD-PJK0TF3"},"source":["\n","---\n","\n","### Name: Isidora Gajic  \n","### Course: AI for Finance  \n","### Assignment 10: Fine-Tuning PLM for Monetary Policy Stance Classification\n","### Date: October 29, 2024  \n","\n","---\n","\n","### Disclaimer:\n","\n","This notebook was developed with the assistance of ChatGPT, an AI language model. While the majority of the code was generated with the help of ChatGPT, the conceptualization of the analysis, selection of specific metrics, and the overall approach were directed by me. I reviewed and tweaked the code to ensure it aligns with the objectives of the assignment and meets the required standards.\n","\n","The analysis and conclusions presented in this document are my own. ChatGPT was used to improve the cohesiveness and fluency of my original writing and to convert text from a Word document into Markdown. All final interpretations, decisions, and the overall approach to the analysis are entirely mine.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"T_p-aR_U0TF5"},"source":["### **Step 0: Background Research**\n","\n","#### **2. Defining Hawkish and Dovish in Monetary Policy**\n","\n","In the context of monetary policy, **\"hawkish\"** and **\"dovish\"** are terms used to describe the stances and attitudes of central banks, like the Federal Reserve, toward inflation, economic growth, and interest rates. These terms influence how financial markets interpret and react to central bank communications.\n","\n","- **Hawkish**:\n","  - **Definition**: A hawkish stance refers to a monetary policy approach that prioritizes controlling inflation over stimulating economic growth. Hawks are concerned that excessive economic growth could lead to high inflation.\n","  - **Characteristics**:\n","    - **Interest Rates**: Advocates for higher interest rates to cool down an overheating economy.\n","    - **Policy Actions**: Supports tightening monetary policy by reducing the money supply or ending asset purchase programs.\n","    - **Implications for Markets**:\n","      - **Bonds**: Higher interest rates can lead to lower bond prices.\n","      - **Stocks**: May negatively affect stock markets due to increased borrowing costs.\n","      - **Currency**: Can strengthen the national currency as higher rates attract foreign investment.\n","\n","- **Dovish**:\n","  - **Definition**: A dovish stance emphasizes stimulating economic growth and reducing unemployment over controlling inflation. Doves are less concerned about inflation and more focused on supporting the economy.\n","  - **Characteristics**:\n","    - **Interest Rates**: Favors lower interest rates to encourage borrowing and investment.\n","    - **Policy Actions**: Supports expanding monetary policy by increasing the money supply or initiating asset purchase programs.\n","    - **Implications for Markets**:\n","      - **Bonds**: Lower interest rates can lead to higher bond prices.\n","      - **Stocks**: Often boosts stock markets due to cheaper borrowing costs.\n","      - **Currency**: May weaken the national currency as lower rates can reduce foreign investment appeal.\n","\n","**Relation to Federal Reserve Communications**:\n","\n","The Federal Reserve uses various communication tools—such as meeting minutes, speeches, and press releases—to signal its monetary policy stance. Market participants closely analyze this language to gauge future policy actions.\n","\n","- **Hawkish Communications**:\n","  - **Signals**: Indicate concerns about inflation and suggest potential interest rate hikes.\n","  - **Market Reaction**: Can lead to increased volatility, as investors adjust expectations for tighter monetary conditions.\n","\n","- **Dovish Communications**:\n","  - **Signals**: Highlight concerns about economic slowdown or unemployment, suggesting that interest rates may remain low or decrease.\n","  - **Market Reaction**: Often leads to stock market rallies and lower bond yields, as investors anticipate more accommodative monetary policy.\n","\n","Understanding these terms is crucial for investors, policymakers, and economists, as they directly impact financial market dynamics and economic forecasting.\n","\n","---\n","\n","#### **3. Key Monetary Policy Events (1996-September 2024)**\n","\n","**1. The Dot-Com Bubble Burst and Federal Reserve Actions (Late 1990s - Early 2000s)**\n","\n","- **Overview**:\n","  - During the late 1990s, the U.S. economy experienced rapid growth, particularly in technology and internet-related stocks, leading to the \"dot-com bubble.\"\n","  - The Federal Reserve, led by Chairman Alan Greenspan, increased the federal funds rate multiple times to prevent the economy from overheating.\n","\n","- **Impact**:\n","  - **Interest Rate Hikes**: Between June 1999 and May 2000, the Fed raised rates six times, from 4.75% to 6.5%.\n","  - **Market Reaction**: The higher interest rates contributed to the bursting of the dot-com bubble in 2000, leading to significant stock market losses and an economic slowdown.\n","\n","**2. Federal Reserve Response to the 9/11 Attacks (2001)**\n","\n","- **Overview**:\n","  - The September 11, 2001 terrorist attacks caused immediate economic uncertainty and financial market disruptions.\n","  - The Federal Reserve acted swiftly to stabilize the economy.\n","\n","- **Impact**:\n","  - **Emergency Rate Cuts**: The Fed cut the federal funds rate from 3.5% to 3% shortly after the attacks and continued reducing it to 1.75% by the end of 2001.\n","  - **Liquidity Measures**: Implemented policies to ensure liquidity in financial markets.\n","  - **Economic Support**: These actions helped restore confidence and supported economic recovery.\n","\n","**3. The 2008 Financial Crisis and Quantitative Easing (2007-2009)**\n","\n","- **Overview**:\n","  - A collapse in the U.S. housing market led to a global financial crisis, with major financial institutions facing insolvency.\n","  - The Federal Reserve, under Chairman Ben Bernanke, took unprecedented actions.\n","\n","- **Impact**:\n","  - **Interest Rates**: Reduced the federal funds rate to near zero (0-0.25%) by December 2008.\n","  - **Quantitative Easing (QE)**:\n","    - **QE1 (2008-2010)**: The Fed began purchasing large amounts of mortgage-backed securities and Treasuries to inject liquidity.\n","    - **Objective**: Lower long-term interest rates, support mortgage lending, and stimulate the economy.\n","  - **Market Reaction**: These measures helped stabilize financial markets but also raised concerns about long-term inflation and asset bubbles.\n","\n","**4. Monetary Policy Response to the COVID-19 Pandemic (2020)**\n","\n","- **Overview**:\n","  - The COVID-19 pandemic led to a sudden economic shutdown and a sharp downturn.\n","\n","- **Impact**:\n","  - **Emergency Rate Cuts**: In March 2020, the Fed cut the federal funds rate to 0-0.25%.\n","  - **Quantitative Easing**:\n","    - Announced unlimited QE to purchase Treasury securities and mortgage-backed securities.\n","  - **Additional Measures**:\n","    - **Emergency Lending Facilities**: Established programs to support businesses, municipalities, and financial markets.\n","    - **Forward Guidance**: Signaled that rates would remain low until the economy showed signs of recovery.\n","  - **Market Reaction**: Stabilized financial markets and supported economic activity, but raised concerns about long-term debt and inflation.\n","\n","**5. Inflation Surge and Policy Shift Toward Tightening (2021-2023)**\n","\n","- **Overview**:\n","  - Post-pandemic recovery led to supply chain disruptions, labor shortages, and significant fiscal stimulus, contributing to rising inflation.\n","\n","- **Impact**:\n","  - **Shift in Stance**: The Fed signaled a move from a dovish to a more hawkish stance to address inflation.\n","  - **Interest Rate Increases**:\n","    - Began raising rates in 2022, with multiple hikes throughout the year.\n","  - **Reducing Balance Sheet**:\n","    - Initiated plans to reduce the Fed's balance sheet by ceasing reinvestments.\n","  - **Market Reaction**:\n","    - Increased market volatility, concerns over potential economic slowdown, and adjustments in asset valuations.\n","\n","#### **6. Federal Reserve's Half-Percentage Point Rate Cut (September 2024)**\n","\n","- **Overview**:\n","  - On **September 18, 2024**, the Federal Reserve cut the federal funds rate by **half a percentage point**, bringing it to a range between **4.75% and 5%**.\n","  - This was the **first rate cut since 2020**, marking a significant shift from the Fed's previous focus on combating inflation to supporting the labor market.\n","  - The decision was more aggressive than most analysts anticipated; many expected a smaller **quarter-point** reduction.\n","\n","- **Impact**:\n","  - **Monetary Policy Shift**:\n","    - The rate cut signaled the Fed's commitment to preventing a gentle cooling in the labor market from turning into a deeper slowdown.\n","    - Fed Chair **Jerome Powell** stated that the decision reflects growing confidence in maintaining economic strength with an appropriate policy recalibration.\n","  - **Market Reaction**:\n","    - **Immediate Relief**: The cut provided immediate relief to consumers with credit-card balances and small businesses with variable-rate debt.\n","    - **Stock Market**: Stocks initially rose following the announcement but ended the day lower, indicating mixed investor sentiment.\n","    - **Bond Market**: Long-term borrowing costs had been declining in anticipation of rate cuts, affecting mortgages and corporate debt.\n","  - **Economic Indicators**:\n","    - **Unemployment Rate**: Rose to **4.2%**, up from **3.7%** in January 2024, indicating a softening labor market.\n","    - **Inflation**: Had fallen over the past year, reducing pressure on the Fed to maintain higher interest rates.\n","  - **Policy Implications**:\n","    - **Future Rate Cuts**: Projections indicated potential additional cuts in November and December 2024.\n","    - **Risk Management**: The larger-than-expected cut was seen as a proactive measure to mitigate the risk of an economic downturn.\n","    - **Fed Communications**: Emphasized maintaining the strength of the labor market and the overall economy through adjusted monetary policy.\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"ddBhzczI0TF6"},"source":["### Importing Necessary Libraries\n","\n","The following libraries are imported for data manipulation, model training, and evaluation:\n","\n","- **pandas** and **numpy** for data handling.\n","- **matplotlib** and **seaborn** for data visualization.\n","- **torch** and **torch.utils.data** for building datasets.\n","- **transformers** from Hugging Face for model and tokenizer.\n","- **sklearn** for evaluation metrics and data splitting.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":46514,"status":"ok","timestamp":1729811940361,"user":{"displayName":"Isidora Gajic","userId":"01275553670441520985"},"user_tz":240},"id":"9huNwT4i0TF7"},"outputs":[],"source":["# Importing necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# PyTorch libraries\n","import torch\n","from torch.utils.data import Dataset\n","\n","# Hugging Face transformers\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n","\n","# Sklearn for evaluation metrics and data splitting\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","\n","# Set random seed for reproducibility\n","import random\n","import os\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(0)\n"]},{"cell_type":"markdown","metadata":{"id":"HHYn7LZg0TF7"},"source":["### Loading the Data\n","\n","The labeled datasets for hawkish-dovish classification are loaded:\n","\n","- **Training Data**: `lab-manual-mm-train-5768.xlsx`\n","- **Testing Data**: `lab-manual-mm-test-5768.xlsx`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":178,"status":"error","timestamp":1729812223442,"user":{"displayName":"Isidora Gajic","userId":"01275553670441520985"},"user_tz":240},"id":"OV0f8dze0TF8","outputId":"f8a89ee9-e3db-4de6-ce3c-4857ba4b51eb"},"outputs":[],"source":["# Loading the training data\n","train_df = pd.read_excel('/content/drive/My Drive/Computers/My Mac/Desktop/AI_for_Finance/Assignment 10/lab-manual-mm-train-5768.xlsx')\n","\n","# Loading the testing data\n","test_df = pd.read_excel('/content/drive/My Drive/Computers/My Mac/Desktop/AI_for_Finance/Assignment 10/lab-manual-mm-test-5768.xlsx')\n","\n","# Display the columns and number of samples\n","print(\"Columns in train_df:\", train_df.columns.tolist())\n","print(\"Columns in test_df:\", test_df.columns.tolist())\n","\n","print(\"Number of samples in train_df:\", len(train_df))\n","print(\"Number of samples in test_df:\", len(test_df))\n"]},{"cell_type":"markdown","metadata":{"id":"SFry_9I_0TF8"},"source":["### Data Preprocessing\n","\n","- **Label Mapping**: Ensure labels are integers for model compatibility.\n","- **Missing Values**: Checked for any missing values and dropped them to ensure data integrity.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNFGIIh20TF8"},"outputs":[],"source":["# Ensure that 'label' is of integer type\n","train_df['label'] = train_df['label'].astype(int)\n","test_df['label'] = test_df['label'].astype(int)\n","\n","# Check for missing values in 'sentence' and 'label' columns\n","print(\"Training Data Missing Values:\\n\", train_df[['sentence', 'label']].isnull().sum())\n","print(\"Testing Data Missing Values:\\n\", test_df[['sentence', 'label']].isnull().sum())\n","\n","# Drop rows with missing values in 'sentence' or 'label'\n","train_df = train_df.dropna(subset=['sentence', 'label'])\n","test_df = test_df.dropna(subset=['sentence', 'label'])\n","\n","# Verify the number of samples after cleaning\n","print(\"Number of samples in train_df after cleaning:\", len(train_df))\n","print(\"Number of samples in test_df after cleaning:\", len(test_df))\n","\n","# Optional: Reset index after dropping rows\n","train_df = train_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n"]},{"cell_type":"markdown","metadata":{"id":"81tTXwh80TF8"},"source":["### Exploring Label Distribution\n","\n","- Visualized the distribution of labels in the training data to check for class imbalance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tys8owCT0TF9"},"outputs":[],"source":["# Explore label distribution in training data\n","label_counts = train_df['label'].value_counts()\n","print(\"Label distribution in training data:\\n\", label_counts)\n","\n","# Plotting label distribution\n","plt.figure(figsize=(6,4))\n","sns.barplot(x=label_counts.index, y=label_counts.values)\n","plt.title('Label Distribution in Training Data')\n","plt.xlabel('Labels')\n","plt.ylabel('Count')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"QXmVp5CD0TF9"},"source":["### Setting Up the RoBERTa Model\n","\n","- **Tokenizer**: Loaded `RobertaTokenizer` for tokenizing the text data.\n","- **Model**: Loaded `RobertaForSequenceClassification` with 3 output labels corresponding to hawkish, dovish, and neutral classes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53K-l6670TF9"},"outputs":[],"source":["# Loading the tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","# Loading the pre-trained model for sequence classification\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n"]},{"cell_type":"markdown","metadata":{"id":"DYJ456DV0TF9"},"source":["### Tokenization\n","\n","The tokenizer is configured with the following parameters:\n","\n","- `padding='max_length'`: Pads all sequences to the maximum length specified.\n","- `truncation=True`: Truncates sequences longer than the maximum length.\n","- `max_length=256`: Sets the maximum sequence length.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZevgNA1T0TF9"},"outputs":[],"source":["# Tokenization parameters\n","tokenizer_kwargs = {\n","    'padding': 'max_length',  # Pad sequences to the maximum length\n","    'truncation': True,       # Truncate sequences longer than the maximum length\n","    'max_length': 256,        # Maximum sequence length\n","    'return_tensors': 'pt'    # Return PyTorch tensors\n","}\n"]},{"cell_type":"markdown","metadata":{"id":"YNQjAP-w0TF9"},"source":["**Why is tokenization important for models like RoBERTa?**\n","\n","Tokenization is crucial for models like RoBERTa because it transforms raw text into numerical input IDs that the model can process. RoBERTa uses Byte-Pair Encoding (BPE) to handle rare and sub-word tokens, which allows the model to understand and represent words that may not be in its vocabulary. Proper tokenization ensures:\n","\n","- **Consistency**: Text data is consistently formatted, allowing the model to learn effectively.\n","- **Context Preservation**: Subword tokenization preserves the meaning of words in context.\n","- **Efficiency**: Fixed-length inputs allow for efficient batch processing.\n","- **Model Compatibility**: The model expects inputs in a specific tokenized format.\n","\n","---\n","\n","### Preparing the Custom Dataset\n","\n","A custom `FOMCDataset` class is defined to:\n","\n","- **Initialize**: Store texts, labels, tokenizer, and tokenization parameters.\n","- **`__len__`**: Return the total number of samples.\n","- **`__getitem__`**: Retrieve an item by index, tokenize the text, and return input tensors and labels.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZczP5TEB0TF-"},"outputs":[],"source":["class FOMCDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, tokenizer_kwargs):\n","        self.texts = texts.reset_index(drop=True)\n","        self.labels = labels.reset_index(drop=True)\n","        self.tokenizer = tokenizer\n","        self.tokenizer_kwargs = tokenizer_kwargs\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer(text, **self.tokenizer_kwargs)\n","\n","        # Remove the extra dimension added by 'return_tensors' for input_ids and attention_mask\n","        item = {key: val.squeeze(0) for key, val in encoding.items()}\n","        item['labels'] = torch.tensor(label, dtype=torch.long)\n","\n","        return item\n"]},{"cell_type":"markdown","metadata":{"id":"t9RSewT30TF-"},"source":["### Splitting the Data\n","\n","- **Data Split**: The training data is split into training and validation sets (80/20 split).\n","- **Labels**: Ensured that labels are integers for model compatibility.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIF3e2TX0TF-"},"outputs":[],"source":["# Splitting the training data into training and validation sets (80/20 split)\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    train_df['sentence'],\n","    train_df['label'],\n","    test_size=0.2,\n","    random_state=0\n",")\n","\n","# Ensure labels are integers\n","train_labels = train_labels.astype(int)\n","val_labels = val_labels.astype(int)\n","test_labels = test_df['label'].astype(int)\n"]},{"cell_type":"markdown","metadata":{"id":"UnTtTUKz0TF-"},"source":["### Creating Dataset Instances\n","\n","- **Datasets**: Instances of `FOMCDataset` are created for training, validation, and testing data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kaFos9B0TF-"},"outputs":[],"source":["# Creating dataset instances\n","train_dataset = FOMCDataset(train_texts, train_labels, tokenizer, tokenizer_kwargs)\n","val_dataset = FOMCDataset(val_texts, val_labels, tokenizer, tokenizer_kwargs)\n","test_dataset = FOMCDataset(test_df['sentence'], test_labels, tokenizer, tokenizer_kwargs)\n"]},{"cell_type":"markdown","metadata":{"id":"nsv1ltB50TF-"},"source":["### Defining Evaluation Metrics\n","\n","- **Purpose**: Define a function to compute the weighted F1 score during evaluation.\n","- **Weighted F1 Score**: Accounts for class imbalance by weighting classes according to their presence.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwFHawpK0TF-"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=1)\n","    f1 = f1_score(labels, predictions, average='weighted')\n","    return {'weighted_f1': f1}\n"]},{"cell_type":"markdown","metadata":{"id":"hr2TEEQZ0TF-"},"source":["### Hyperparameter Optimization\n","\n","Performed a grid search over the following hyperparameters:\n","\n","- **Learning Rates**: `[5e-5, 3e-5, 2e-5, 1e-5]`\n","- **Batch Sizes**: `[4, 8, 16]`\n","- **Epochs**: `[3, 5, 10]`\n","\n","The `Trainer` from Hugging Face is used to train the model with different hyperparameter combinations, optimizing for the highest weighted F1 score.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0U_F1PUR0TF-"},"outputs":[],"source":["# Hyperparameter ranges\n","learning_rates = [5e-5, 3e-5, 2e-5, 1e-5]\n","batch_sizes = [4, 8, 16]\n","num_epochs = [3, 5, 10]\n","\n","# For tracking results\n","results = []\n","\n","# Loop over hyperparameters\n","for lr in learning_rates:\n","    for batch_size in batch_sizes:\n","        for epochs in num_epochs:\n","            print(f\"\\nTraining with learning_rate={lr}, batch_size={batch_size}, epochs={epochs}\")\n","\n","            # Re-initialize the model for each run\n","            model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n","\n","            # Update training arguments\n","            training_args = TrainingArguments(\n","                output_dir='./results',\n","                num_train_epochs=epochs,\n","                per_device_train_batch_size=batch_size,\n","                per_device_eval_batch_size=batch_size,\n","                evaluation_strategy='epoch',\n","                save_strategy='no',\n","                logging_strategy='epoch',\n","                learning_rate=lr,\n","                load_best_model_at_end=False,\n","                seed=0\n","            )\n","\n","            # Initialize the Trainer\n","            trainer = Trainer(\n","                model=model,\n","                args=training_args,\n","                train_dataset=train_dataset,\n","                eval_dataset=val_dataset,\n","                compute_metrics=compute_metrics,\n","                tokenizer=tokenizer\n","            )\n","\n","            # Train the model\n","            trainer.train()\n","\n","            # Evaluate on validation set\n","            eval_result = trainer.evaluate()\n","            weighted_f1 = eval_result['eval_weighted_f1']\n","            print(f\"Validation Weighted F1 Score: {weighted_f1:.4f}\")\n","\n","            results.append({\n","                'learning_rate': lr,\n","                'batch_size': batch_size,\n","                'epochs': epochs,\n","                'weighted_f1': weighted_f1\n","            })\n"]},{"cell_type":"markdown","metadata":{"id":"rBAHw24W0TF-"},"source":["### Analyzing Hyperparameter Results\n","\n","- **Best Hyperparameters**: Identified based on the highest weighted F1 score from the grid search.\n","  - **Learning Rate**: `{best_learning_rate}`\n","  - **Batch Size**: `{best_batch_size}`\n","  - **Epochs**: `{best_num_epochs}`\n","- **Results Summary**: Displayed all hyperparameter combinations and their corresponding performance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_JtLXZx0TF-"},"outputs":[],"source":["# Convert results to DataFrame\n","results_df = pd.DataFrame(results)\n","\n","# Find the best hyperparameters\n","best_row = results_df.loc[results_df['weighted_f1'].idxmax()]\n","best_learning_rate = best_row['learning_rate']\n","best_batch_size = best_row['batch_size']\n","best_num_epochs = best_row['epochs']\n","\n","print(\"\\nBest Hyperparameters:\")\n","print(f\"Learning Rate: {best_learning_rate}\")\n","print(f\"Batch Size: {best_batch_size}\")\n","print(f\"Epochs: {best_num_epochs}\")\n","print(f\"Best Weighted F1 Score: {best_row['weighted_f1']:.4f}\")\n","\n","# Display the results DataFrame\n","print(\"\\nHyperparameter Tuning Results:\")\n","print(results_df.sort_values(by='weighted_f1', ascending=False))\n"]},{"cell_type":"markdown","metadata":{"id":"7V3qEE4U0TF-"},"source":["### Training the Final Model\n","\n","- **Best Hyperparameters**: Used the optimal settings identified earlier to train the final model.\n","- **Training Data**: Combined training and validation sets for final training.\n","- **Model Saving**: Configured to save the best model based on the evaluation metric.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUD9npa30TF-"},"outputs":[],"source":["# Combine training and validation data\n","final_train_texts = pd.concat([train_texts, val_texts]).reset_index(drop=True)\n","final_train_labels = pd.concat([train_labels, val_labels]).reset_index(drop=True)\n","\n","# Create final training dataset\n","final_train_dataset = FOMCDataset(final_train_texts, final_train_labels, tokenizer, tokenizer_kwargs)\n","\n","# Re-initialize the model\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n","\n","# Update training arguments with best hyperparameters\n","training_args = TrainingArguments(\n","    output_dir='./best_model',\n","    num_train_epochs=int(best_num_epochs),\n","    per_device_train_batch_size=int(best_batch_size),\n","    per_device_eval_batch_size=int(best_batch_size),\n","    evaluation_strategy='epoch',\n","    save_strategy='epoch',\n","    logging_strategy='epoch',\n","    learning_rate=best_learning_rate,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='weighted_f1',\n","    seed=0\n",")\n","\n","# Initialize the Trainer with best hyperparameters\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=final_train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer\n",")\n","\n","# Train the final model\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"d3VHMaMr0TF_"},"source":["### Evaluating the Model\n","\n","- **Predictions**: Made on the test dataset.\n","- **Metrics Computed**:\n","  - **Weighted F1 Score**: Evaluates the overall performance.\n","  - **Classification Report**: Provides precision, recall, and F1 score for each class.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARAJ87Qk0TF_"},"outputs":[],"source":["# Make predictions on the test set\n","predictions_output = trainer.predict(test_dataset)\n","predictions = predictions_output.predictions\n","labels = predictions_output.label_ids\n","predicted_labels = np.argmax(predictions, axis=1)\n","\n","# Compute the weighted F1 score\n","test_f1 = f1_score(labels, predicted_labels, average='weighted')\n","print(f'\\nWeighted F1 Score on Test Set: {test_f1:.4f}')\n","\n","# Generate classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(labels, predicted_labels, target_names=['Dovish', 'Hawkish', 'Neutral']))\n"]},{"cell_type":"markdown","metadata":{"id":"OFQwWDqd0TF_"},"source":["### Confusion Matrix Analysis\n","\n","- **Visualization**: The confusion matrix is plotted to visualize the model's performance across classes.\n","- **Insights**:\n","  - **Diagonal Elements**: Represent correct predictions.\n","  - **Off-Diagonal Elements**: Represent misclassifications.\n","- **Interpretation**:\n","  - Analyzed which classes are often confused with each other.\n","  - Identified if the model is biased towards a particular class.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRPZnU2k0TF_"},"outputs":[],"source":["# Confusion matrix\n","cm = confusion_matrix(labels, predicted_labels)\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(8,6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Dovish', 'Hawkish', 'Neutral'], yticklabels=['Dovish', 'Hawkish', 'Neutral'])\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"EpDKH7EI0TF_"},"source":["### Hyperparameter Analysis\n","\n","- **Learning Rate**:\n","  - **Higher Learning Rates**: May cause the model to converge quickly but can overshoot optimal solutions.\n","  - **Lower Learning Rates**: Provide more stable convergence but may require more epochs.\n","- **Batch Size**:\n","  - **Smaller Batch Sizes**: Lead to more updates and can help in generalization but increase training time.\n","  - **Larger Batch Sizes**: Reduce training time but may lead to less generalization.\n","- **Number of Epochs**:\n","  - **Fewer Epochs**: May result in underfitting.\n","  - **More Epochs**: Can improve performance but risk overfitting.\n","- **Observations**:\n","  - **Optimal Combination**: The best performance was achieved with a learning rate of `{best_learning_rate}`, batch size of `{best_batch_size}`, and `{best_num_epochs}` epochs.\n","  - **Performance Trends**: Described any trends observed during hyperparameter tuning.\n"]},{"cell_type":"markdown","metadata":{"id":"3nFo2hD90TF_"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
